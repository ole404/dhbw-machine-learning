{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset eli5 (/Users/ole/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5\", split=\"train_asks[:1000]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'q_id': '2xreok',\n 'title': 'Do we still use real mode for anything?',\n 'selftext': 'Does this happen or everything is in virtual mode nowadays?',\n 'document': '',\n 'subreddit': 'askscience',\n 'answers.a_id': ['cp31nef'],\n 'answers.text': ['Can you explain?  [Real mode](_URL_0_) usually refers to the 16 bit mode of x86 processors.  In that sense, any x86 machine that is not using EFI will boot into real mode (and rapidly switch to 32 or 64 bit protected mode).'],\n 'answers.score': [5],\n 'title_urls.url': [],\n 'selftext_urls.url': [],\n 'answers_urls.url': ['http://en.wikipedia.org/wiki/Real_mode']}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5 = eli5.train_test_split(test_size=0.2)\n",
    "eli5[\"train\"][0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]], truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0e9406566eb4ed78768ff1a5d5fb1fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#3:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb6639b61897487f9a50678f9cd4bf75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afe819de2346420e95082424f1e1ca8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#2:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed8d9d06c0514526a0c599af5920d993"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "text/plain": "#2:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "328dd0f81c5b48eab864ae022f6d9908"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5f563a5ed814e7aae542b98ce9d9c30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "921c59cf9d7f42ef82c851e80be6c34d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#3:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "555223d106e84fd68f6bc81c139da669"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39b44a7aa4c746a7a52a04d41bc5d839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e02e82931d8a4d1894bf8f7a08453174"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#2:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d8e1dd1fffb4bd4bad0003d85e3709f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#3:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08408ea9e9e7467aaf77c5f863bdb673"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59d72cae87c14115876d03acabc6f97a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8162cfc8ee094adc95cd1692e84506e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#2:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee8c1cd05864daa9ddf3b0e9a3413e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "#3:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fe9fb8c16e949f2889b62d81f53f54b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "\" after they parse it, if you look at a rainbow, ROYGBIV, light at the red end of the spectrum is 'heavier', it moves more slowly. At the blue end it's 'lighter', it moves more quickly. Now they can cancel the OYG portion. They're making those beams hit your eye (or in this case, a detection grid) out of synch. So instead the red stream will get confused with other background events going on at the time, etc.\\n\\nIt's optical. The only reason time is mentioned is because there's a speed difference between the light frequencies.\""
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode (lm_dataset[\"train\"][-1][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "'It\\'s a fascinatingly simple idea. \\n\\nA light pulse has a certain bandwidth (i.e. it contains a bunch of different frequencies), which is larger the shorter the pulse is. \\n\\nIn this time-cloaking experiment, they \"pull apart\" this light pulse by accelerating the blue part of the spectrum and slowing down the red part of it, creating a spectral hole in the middle. This can be achieved by cleverly engineering an optical material\\'s dispersion curve, i.e. the frequency-dependent refractive index experienced by the light pulse. \\n\\nNow that you have that spectral hole, you can put an event inside which is now \"cloaked\" (not really, but ok). Put the pulse together afterwards by reversing the initial dispersion and it will now look like that event in the middle never happened, the light pulse comes out exactly like it went in.\\n\\nIf you want more technical details, let me know. All due respect to PhysOrg... but it\\'s a sensationalist bullshit title.\\n\\n\"The fibre-based system steers light ‘around’ an event so that no evidence (a change in the temporal or spectral properties of the light beam) of the event is detectable, by speeding up and slowing down different parts of a light beam.\"\\n\\nThe illusion works purely on manipulating the flow of light. There\\'s no dimensional rift, there\\'s no erasing of time. It\\'s about perception, not [time warps](_URL_0_).\\n\\nBecause fiber optics can carry multiple frequencies of light on the same physical medium, you can [split red and blue light streams with a special lens](_URL_1_). This means they\\'ll hit an observer\\'s eye at different times. So after they parse it, if you look at a rainbow, ROYGBIV, light at the red end of the spectrum is \\'heavier\\', it moves more slowly. At the blue end it\\'s \\'lighter\\', it moves more quickly. Now they can cancel the OYG portion. They\\'re making those beams hit your eye (or in this case, a detection grid) out of synch. So instead the red stream will get confused with other background events going on at the time, etc.\\n\\nIt\\'s optical. The only reason time is mentioned is because there\\'s a speed difference between the light frequencies.'"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_eli5[\"train\"][-1][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollator\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "tf_train_set = lm_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    dummy_labels=True,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(16, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(16,), dtype=tf.float32, name=None))>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 1698\n})"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset['train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 20:57:36.246956: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n",
      "FUck tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 21:01:38.893716: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 707, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 267, in constant\n",
      "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 279, in _constant_impl\n",
      "    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_eager_impl\n",
      "    t = convert_to_eager_tensor(value, ctx, dtype)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "\n",
      "ValueError: Can't convert non-rectangular Python sequence to Tensor.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 404, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 40, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 692, in tf_call\n",
      "    batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2876, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 209, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 723, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 707, in convert_to_tensors\n    tensor = as_tensor(value)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Can't convert non-rectangular Python sequence to Tensor.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 404, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 40, in __call__\n    return self.tf_call(features)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 692, in tf_call\n    batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2876, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 209, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 723, in convert_to_tensors\n    raise ValueError(\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n\n\n\t [[{{function_node __inference_fetch_function_4707}}{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[0;32mIn [133]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tf_train_set:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFUck tensorflow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:836\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    835\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 836\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    837\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOutOfRangeError:\n\u001B[1;32m    838\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:819\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecution_mode(context\u001B[38;5;241m.\u001B[39mSYNC):\n\u001B[0;32m--> 819\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator_get_next\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    820\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    821\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    824\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    825\u001B[0m     \u001B[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[1;32m    826\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_element_spec\u001B[38;5;241m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2923\u001B[0m, in \u001B[0;36miterator_get_next\u001B[0;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[1;32m   2921\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   2922\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 2923\u001B[0m   \u001B[43m_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_from_not_ok_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_FallbackException:\n\u001B[1;32m   2925\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7186\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   7184\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name):\n\u001B[1;32m   7185\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 7186\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 707, in convert_to_tensors\n    tensor = as_tensor(value)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Can't convert non-rectangular Python sequence to Tensor.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 404, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 40, in __call__\n    return self.tf_call(features)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 692, in tf_call\n    batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2876, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 209, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n\n  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 723, in convert_to_tensors\n    raise ValueError(\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n\n\n\t [[{{function_node __inference_fetch_function_4707}}{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "for i in tf_train_set:\n",
    "    print('FUck tensorflow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 20:58:00.931613: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 707, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 267, in constant\n",
      "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 279, in _constant_impl\n",
      "    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_eager_impl\n",
      "    t = convert_to_eager_tensor(value, ctx, dtype)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in convert_to_eager_tensor\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "\n",
      "ValueError: Can't convert non-rectangular Python sequence to Tensor.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 404, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 40, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 692, in tf_call\n",
      "    batch = self.tokenizer.pad(examples, return_tensors=\"tf\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2876, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 209, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 723, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/106 [=>............................] - ETA: 14:06 - loss: 3.9368"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [131]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_train_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py:1384\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1379\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1380\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1381\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1382\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1383\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1384\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1385\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1386\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2953\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2954\u001B[0m   (graph_function,\n\u001B[1;32m   2955\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2956\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2957\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1849\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1851\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1852\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1853\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1854\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1855\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1856\u001B[0m     args,\n\u001B[1;32m   1857\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1858\u001B[0m     executing_eagerly)\n\u001B[1;32m   1859\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}